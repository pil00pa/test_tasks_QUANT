{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generated data using ChatGPT, resulting in 998 short sentences  \n",
    "where mountain names are marked/tagged. Let's look at some example  \n",
    "sentences and the format they are written in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of structure:  {'text': 'Mount Everest stands as the highest peak in the world at 8,848 meters.', 'labels': {'mountain': 'Mount Everest'}}\n",
      "\n",
      "Sentence 1: Mount Everest stands as the highest peak in the world at 8,848 meters.\n",
      "Labels: Mount Everest\n",
      "\n",
      "Sentence 2: Climbers spent three weeks ascending the dangerous slopes of K2.\n",
      "Labels: K2\n",
      "\n",
      "Sentence 3: The Rocky Mountains stretch from British Columbia to New Mexico.\n",
      "Labels: Rocky Mountains\n",
      "\n",
      "Sentence 4: Tourists often visit Mont Blanc to enjoy skiing in the winter.\n",
      "Labels: Mont Blanc\n",
      "\n",
      "Sentence 5: The Andes Mountains form the longest mountain range in South America.\n",
      "Labels: Andes Mountains\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and load the raw dataset from a JSON file\n",
    "f = open('raw_dataset.json', 'r', encoding='utf-8')\n",
    "data = json.load(f)\n",
    "\n",
    "\n",
    "print(\"Example of structure: \", data['sentences'][0])\n",
    "print()\n",
    "\n",
    "# Print the first 5 sentences from the loaded data\n",
    "for i, sentence_data in enumerate(data['sentences'][:5]):\n",
    "    print(f\"Sentence {i+1}: {sentence_data['text']}\")\n",
    "    print(f\"Labels:\", sentence_data['labels']['mountain'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use a lightweight version of the BERT model — DistilBERT.  \n",
    "Since my dataset is small and the task is relatively simple. To train  \n",
    "the model, we need to convert the data into an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Load the raw dataset from JSON file\n",
    "\n",
    "f = open('raw_dataset.json', 'r', encoding='utf-8')\n",
    "data = json.load(f)\n",
    "\n",
    "def convert_data_to_bert_format(data):\n",
    "    \"\"\"\n",
    "    Convert raw NER data into BERT-compatible BIO format.\n",
    "    \n",
    "    BIO Format:\n",
    "    - B-TAG: Beginning of an entity\n",
    "    - I-TAG: Inside/continuation of an entity\n",
    "    - O: Outside any entity\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Raw data containing sentences and their entity labels\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing tokenized sentences and their BIO labels\n",
    "    \"\"\"\n",
    "    converted_data = []\n",
    "    \n",
    "    # Process each sentence in the dataset\n",
    "    for sentence_data in data['sentences']:\n",
    "        # Tokenize the sentence into words and punctuation\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', sentence_data['text'])\n",
    "        \n",
    "        # Initialize all words with 'O' (Outside) label\n",
    "        word_labels = [\"O\"] * len(words)\n",
    "        \n",
    "        # Process each entity and its corresponding label in the sentence\n",
    "        for entity, entity_name in sentence_data['labels'].items():\n",
    "            # Handle both string and list entity names\n",
    "            # Some datasets might have entity names as strings, others as lists\n",
    "            if isinstance(entity_name, str):\n",
    "                entity_words = entity_name.split()\n",
    "            else:\n",
    "                entity_words = entity_name\n",
    "                \n",
    "            # Find and label all occurrences of the entity in the sentence\n",
    "            for i in range(len(words) - len(entity_words) + 1):\n",
    "                # Check if we found the entity at current position\n",
    "                if words[i:i + len(entity_words)] == entity_words:\n",
    "                    # Label the first word of entity with B- (Beginning)\n",
    "                    word_labels[i] = f\"B-{entity.upper()}\"\n",
    "                    \n",
    "                    # Label subsequent words with I- (Inside)\n",
    "                    for j in range(1, len(entity_words)):\n",
    "                        word_labels[i + j] = f\"I-{entity.upper()}\"\n",
    "        \n",
    "        # Store the processed sentence and its labels\n",
    "        converted_data.append({\n",
    "            \"sentence\": words,          # Tokenized words\n",
    "            \"labels\": word_labels       # Corresponding BIO labels\n",
    "        })\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "# Convert the data to BERT format and save it\n",
    "converted_data = convert_data_to_bert_format(data)\n",
    "\n",
    "# Save the processed data to a new JSON file\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(converted_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create and train the DistilBert model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model for 5 epochs, which is generally  \n",
    "sufficient for such a small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 25/125 [00:04<00:18,  5.39it/s]\n",
      " 21%|██        | 26/125 [00:05<00:30,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07221564650535583, 'eval_runtime': 0.4126, 'eval_samples_per_second': 484.71, 'eval_steps_per_second': 16.965, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 50/125 [00:09<00:13,  5.41it/s]\n",
      " 41%|████      | 51/125 [00:10<00:22,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029547180980443954, 'eval_runtime': 0.4025, 'eval_samples_per_second': 496.839, 'eval_steps_per_second': 17.389, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 75/125 [00:14<00:09,  5.39it/s]\n",
      " 61%|██████    | 76/125 [00:15<00:14,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.022712137550115585, 'eval_runtime': 0.4029, 'eval_samples_per_second': 496.378, 'eval_steps_per_second': 17.373, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 100/125 [00:19<00:04,  5.39it/s]\n",
      " 81%|████████  | 101/125 [00:20<00:07,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.020518431439995766, 'eval_runtime': 0.4015, 'eval_samples_per_second': 498.139, 'eval_steps_per_second': 17.435, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:25<00:00,  5.41it/s]\n",
      "100%|██████████| 125/125 [00:27<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.023718249052762985, 'eval_runtime': 0.3686, 'eval_samples_per_second': 542.623, 'eval_steps_per_second': 18.992, 'epoch': 5.0}\n",
      "{'train_runtime': 27.4676, 'train_samples_per_second': 145.262, 'train_steps_per_second': 4.551, 'train_loss': 0.07670320892333984, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.07670320892333984, metrics={'train_runtime': 27.4676, 'train_samples_per_second': 145.262, 'train_steps_per_second': 4.551, 'total_flos': 22400266581000.0, 'train_loss': 0.07670320892333984, 'epoch': 5.0})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Data Loading and Preparation\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# JSON file with training data\n",
    "file_path = \"data.json\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "def convert_to_dataset(data):\n",
    "    \"\"\"\n",
    "    Convert raw JSON data into format suitable for training.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of dictionaries containing sentences and their NER labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (tokenized_data, labels) where:\n",
    "            - tokenized_data: List of sentences\n",
    "            - labels: List of corresponding NER tags\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    labels = []\n",
    "    for item in data:\n",
    "        tokenized_data.append(item[\"sentence\"])\n",
    "        labels.append(item[\"labels\"])\n",
    "    return tokenized_data, labels\n",
    "\n",
    "# Convert data into required format\n",
    "sentences, ner_tags = convert_to_dataset(data)\n",
    "\n",
    "# Create label mapping for converting string labels to integers\n",
    "label_list = sorted(set(tag for tags in ner_tags for tag in tags))\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "def align_labels_with_tokens(tokenizer, sentence, labels):\n",
    "    \"\"\"\n",
    "    Align NER labels with tokenized input, handling subword tokenization.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        sentence (list): List of input sentences\n",
    "        labels (list): List of NER labels for each sentence\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tokenized inputs with aligned labels\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, \n",
    "                               is_split_into_words=True, return_tensors=\"pt\")\n",
    "    labels_enc = []\n",
    "    \n",
    "    # Process each sentence and its labels\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        # Handle subword tokens and special tokens\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens get label -100 (ignored in loss calculation)\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First token of word gets the NER label\n",
    "                label_ids.append(label_map[label[word_idx]])\n",
    "            else:\n",
    "                # Subsequent subword tokens: keep I- labels, ignore B- labels\n",
    "                label_ids.append(label_map[label[word_idx]] \n",
    "                               if label[word_idx].startswith(\"I-\") else -100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels_enc.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels_enc)\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 2. Model and Tokenizer Initialization\n",
    "# Load pre-trained DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\")\n",
    "model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-cased\", \n",
    "                                                        num_labels=len(label_map))\n",
    "\n",
    "# Prepare training data\n",
    "train_data = align_labels_with_tokens(tokenizer, sentences, ner_tags)\n",
    "\n",
    "# Convert to Hugging Face Dataset format and split into train/test\n",
    "dataset = Dataset.from_dict(train_data)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# 3. Training Configuration\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Directory for storing training outputs\n",
    "    eval_strategy=\"epoch\",           # Evaluation strategy\n",
    "    per_device_train_batch_size=32,  # Training batch size\n",
    "    per_device_eval_batch_size=32,   # Evaluation batch size\n",
    "    num_train_epochs=10,             # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay for regularization\n",
    ")\n",
    "\n",
    "# Initialize data collator for handling variable length sequences\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# 4. Trainer Setup\n",
    "# Initialize Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                           # The model to train\n",
    "    args=training_args,                    # Training arguments\n",
    "    train_dataset=train_test_split[\"train\"], # Training data\n",
    "    eval_dataset=train_test_split[\"test\"],   # Evaluation data\n",
    "    data_collator=data_collator,           # Data collator\n",
    "    processing_class=tokenizer,            # Tokenizer for processing inputs\n",
    ")\n",
    "\n",
    "# 5. Model Training and Saving\n",
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model's loss is quite low, which indicates  \n",
    "successful training so far. This suggests that the model has effectively  \n",
    "learned from the data, and now we will proceed to train it on the entire  \n",
    "available dataset to further refine its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 31/320 [00:05<00:55,  5.22it/s]\n",
      " 10%|█         | 33/320 [00:08<02:50,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3928210794110782e-05, 'eval_runtime': 1.8872, 'eval_samples_per_second': 528.83, 'eval_steps_per_second': 16.956, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 63/320 [00:13<00:48,  5.32it/s]\n",
      " 20%|██        | 65/320 [00:15<02:29,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0011436919448897243, 'eval_runtime': 1.8598, 'eval_samples_per_second': 536.621, 'eval_steps_per_second': 17.206, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 95/320 [00:21<00:42,  5.27it/s]\n",
      " 30%|███       | 97/320 [00:23<02:10,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.817624383373186e-05, 'eval_runtime': 1.852, 'eval_samples_per_second': 538.877, 'eval_steps_per_second': 17.279, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 127/320 [00:29<00:36,  5.25it/s]\n",
      " 40%|████      | 129/320 [00:31<01:52,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1501042536110617e-05, 'eval_runtime': 1.8577, 'eval_samples_per_second': 537.213, 'eval_steps_per_second': 17.225, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 159/320 [00:37<00:30,  5.22it/s]\n",
      " 50%|█████     | 161/320 [00:39<01:33,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.148269782599527e-06, 'eval_runtime': 1.8638, 'eval_samples_per_second': 535.46, 'eval_steps_per_second': 17.169, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 191/320 [00:45<00:24,  5.25it/s]\n",
      " 60%|██████    | 193/320 [00:47<01:15,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.085515219136141e-06, 'eval_runtime': 1.8735, 'eval_samples_per_second': 532.705, 'eval_steps_per_second': 17.081, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 223/320 [00:52<00:18,  5.24it/s]\n",
      " 70%|███████   | 225/320 [00:54<00:55,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.510131126764463e-06, 'eval_runtime': 1.8597, 'eval_samples_per_second': 536.639, 'eval_steps_per_second': 17.207, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 255/320 [01:00<00:12,  5.23it/s]\n",
      " 80%|████████  | 257/320 [01:02<00:37,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.104204996721819e-06, 'eval_runtime': 1.8588, 'eval_samples_per_second': 536.909, 'eval_steps_per_second': 17.216, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 287/320 [01:08<00:06,  5.20it/s]\n",
      " 90%|█████████ | 289/320 [01:10<00:18,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.845733423688216e-06, 'eval_runtime': 1.8671, 'eval_samples_per_second': 534.521, 'eval_steps_per_second': 17.139, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 319/320 [01:16<00:00,  5.23it/s]\n",
      "100%|██████████| 320/320 [01:20<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.767094419046771e-06, 'eval_runtime': 1.8329, 'eval_samples_per_second': 544.503, 'eval_steps_per_second': 17.459, 'epoch': 10.0}\n",
      "{'train_runtime': 80.4996, 'train_samples_per_second': 123.976, 'train_steps_per_second': 3.975, 'train_loss': 0.0009172443300485611, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./trained_distilbert_ner/tokenizer_config.json',\n",
       " './trained_distilbert_ner/special_tokens_map.json',\n",
       " './trained_distilbert_ner/vocab.txt',\n",
       " './trained_distilbert_ner/added_tokens.json',\n",
       " './trained_distilbert_ner/tokenizer.json')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model and tokenizer for later use\n",
    "model.save_pretrained(\"./trained_distilbert_ner\")\n",
    "tokenizer.save_pretrained(\"./trained_distilbert_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let's demonstrate the model's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate that the model has learned but still has clear  \n",
    "shortcomings, let's examine two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Mount -- Mountain\n",
      "Fuji -- Mountain\n",
      "is -- \n",
      "Japan's -- \n",
      "iconic -- \n",
      "and -- \n",
      "highest -- \n",
      "mountain, -- \n",
      "known -- \n",
      "for -- \n",
      "its -- \n",
      "symmetrical -- \n",
      "beauty -- \n",
      "and -- \n",
      "cultural -- \n",
      "significance. -- \n",
      "\n",
      "\n",
      "Sentence 2:\n",
      "Softly, -- \n",
      "softly -- \n",
      "crawl, -- \n",
      "snail -- \n",
      "on -- \n",
      "the -- \n",
      "slope -- \n",
      "of -- \n",
      "Mount -- \n",
      "Fuji, -- \n",
      "up -- \n",
      "to -- \n",
      "the -- \n",
      "heights. -- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example sentences for testing NER model\n",
    "bad_sentence = \"Mount Fuji is Japan's iconic and highest mountain, known for its symmetrical beauty and cultural significance.\"\n",
    "good_sentence = \"Softly, softly crawl, snail on the slope of Mount Fuji, up to the heights.\"\n",
    "sentences = [bad_sentence.split(), good_sentence.split()]\n",
    "\n",
    "# Load the trained model and tokenizer from saved path\n",
    "model_path = \"./trained_distilbert_ner\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize input sentences\n",
    "# is_split_into_words=True because we're passing pre-tokenized sentences\n",
    "inputs = tokenizer(sentences, \n",
    "                  padding=True,        # Add padding to make all sequences same length\n",
    "                  truncation=True,     # Truncate sequences that are too long\n",
    "                  is_split_into_words=True, \n",
    "                  return_tensors=\"pt\")  # Return PyTorch tensors\n",
    "\n",
    "# Get model predictions\n",
    "# torch.no_grad() disables gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the most likely label for each token\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# Get the label mapping dictionary from model config\n",
    "# This maps numerical label IDs back to string labels\n",
    "label_map = model.config.id2label\n",
    "\n",
    "# Convert predicted label indices back to actual labels\n",
    "predicted_labels = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # Get word IDs to handle subword tokenization\n",
    "    word_ids = inputs.word_ids(batch_index=i)\n",
    "    predicted_labels_sentence = []\n",
    "    \n",
    "    # Process predictions for each token\n",
    "    for word_id, label_id in zip(word_ids, predictions[i]):\n",
    "        if word_id is not None:  # Skip special tokens (CLS, SEP, PAD)\n",
    "            label = label_map[label_id.item()]\n",
    "            # Only keep the first subword token's prediction for each word\n",
    "            if len(predicted_labels_sentence) <= word_id:\n",
    "                predicted_labels_sentence.append(label)\n",
    "    \n",
    "    predicted_labels.append(predicted_labels_sentence)\n",
    "\n",
    "# Define mapping from model labels to entity types\n",
    "label_to_entity = {\n",
    "    'LABEL_0': \"Mountain\",  # First label type\n",
    "    'LABEL_1': \"Mountain\",  # Second label type\n",
    "    'LABEL_2': \"\"           # No entity\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for i, (sentence, pred_labels) in enumerate(zip(sentences, predicted_labels)):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    for word, pred_label in zip(sentence, pred_labels):\n",
    "        print(f\"{word} -- {label_to_entity[pred_label]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model performed well on the sentence with  \n",
    "a clearer context, but struggled with the second one. These  \n",
    "results will be analyzed in the report file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
